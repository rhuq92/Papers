Direct policy optimization is an approach within reinforcement learning.

The Markov Decision Process is a typical framework within reinforcement learning. It consists of an agent, an environment, and possible actions. The policy \pi describes the actions an agent takes given the state of the system. The agent wants to maximize their reward, V, which starts at some state s_0. We're going to use the standard discounted learning for the optimized variable. 

Tabular dynamic programming approach: 
State s			|		Action a			|	state-action value Q^\pi(s,a)
-----------------------------------------------------------------------------------------------------------------------
			|						|		

/Third column consists of policy and gives the reward. It's a one-step look ahead. If we could magically compute it for every row, we could update our policy to be greedy, and it will converge pretty quickly.
This table is infinitely big, sadly.

The RL approach is to generalize. Instead of querying the entire table, we can take a sampling approach in order to find an optimal policy. 

Policy gradient methods are one such approach. They easily deal with large state/action spaces. You directly parameterize your policy (through, forinstance, a neural network). The gradient can be computed easily with simulation-based methods. You don't need to know the model.

In ML, it is common practice to directly optimize the quantity of interest. 

There are differences between the gradients in RL and SL (supervised learning). We don't care about non-convexity too much in SL, which is not the case in RL. The regions can have exponentially small gradients, with many higher-order regions being flat. 

Problem in exploration: 

Consider 1 action takes you to the left, another to the right. If you act randomly, you have a random walk, meaning it takes exponential time to reach the rightmost state. This holds even for exact gradients (the gradient is given with a closed form). 

Why do RL methods work? This will provide us with insight on how to improve these methods.

In this talk we look at global convergence of RL methods and provide generalization guarantees of nonconvex policy gradient methods. 

We will start with a simple test case (small state spaces and exact gradients) before moving to large state spaces.
	- What's a state space?

A non-convex space is a space which isn't convex. That is, if for all points $x,y \in \mathbb{R}$, and any $\lambda \in [0,1]$, it is true that $\lambda x + (1-\lambda)y \in \mathbb{R}$, then $\mathbb{R}$ is convex.

An iterative method is called \textbf{locally convergent} if successive approximations are guaranteed to approach when the initial approximation is sufficiently close to the solution. An iterative method that converges for an arbitrary initial approximation is called \textbf{globally convergent}.

This talk provides global convergence \& generalization guarantees of nonconvex policy gradient methods.

They start with small state spaces with exact gradients before moving to large state spaces. Dealing with large state spaces means we need to deal with generalization and distribution shift. 

Part I | Small State Spaces \& the "softmax" policy class
****************************************************************
The softmax policy is the simplest way to parameterize the simplex

What's the simplex?
\begin{definition}
	A simplex is a generalization of the notion of a triangle or tetrahedron to arbitrary dimensions.
\end{definition}
A policy class is, essentially, a set of policies.

Softmax policies are of the form
\begin{equation}
\pi_\theta\theta{\Cond{a}{s}}=\frac{\displaystyle \exp(\theta_s,a)}{\displaystyle \sum_{a'}\exp(\theta_s,a')}
\end{equation}
\begin{equation}
\pi_\theta{\Cond{a}{s}}
\end{equation}
The complete class contains every stationary policy. 

The policy optimization problem $\max_{\theta} V^{\pi_{\theta}}(s_0)$ is non-convex. If we can't find an optimal policy class with this simple problem, how can we find them for a more complicated problem?

We're going to avoid sampling and assume we can get the exact gradients (that is, $\nabla V^{\pi_0}(s_0)$ is given exactly.

Before we do that, we need to define the following:

\begin{shaded}
\begin{definition}[Gradient Descent]
Gradient descent is a first-order (that is, it has at most linear error) iterative optimization algorithm for finding the local minumum of a differentiable function.
\end{definition}
\end{shaded}

Suppose we're given a policy $\theta$ such that 
\begin{equation}
V^{\theta}(\mu) = E_{s \sim \mu}\left[V^\theta (s) \right]
\end{equation}
Instead of directly doing gradient descent on the initial state, we do it on some state that has support over all the states ($\mu$). $\mu$ will have a little bit of mass on all of the states. 
\begin{equation}
\theta \leftarrow \theta + \eta \nabla_\theta V^\theta (\mu)
\end{equation}
We will get global convergence in the limit (which makes sense, since we're starting with a state that represents all of the states).
\begin{shaded}
\begin{theorem}[Vanilla Policy-Gradient for Softmax Policy class]
Suppose $\mu$ has support over the entire state space. Then for all states $s$
\begin{equation}
V^\theta(s) \to V^*(s)
\end{equation}
\end{theorem}
\end{shaded}

The rate could be exponentially slow in terms of state and the softmax can have very flat gradients. This means that finding the optimal policy could be exponentially slow in the number of states. It's because the softmax has a very flat gradient. EG if you go to some corner, the gradient could be very slow, so moving away from the corner can take a very long time.

What if we try some sort of regularizatioin-penalty to bias ourselves from getting these flat gradients (possible if the policy is deterministic)? The simplest way to do this is to use log-barrier regularization. 

Instead of doing gradient descent on the spread-out measure, we can add a penalty that pushes us away from determinsitc policies. The average-log probability stops us from forming a policy that's too deterministic. This leads us to optimize the following:
\begin{eqnarray}
L_\lambda(\theta) &=& V^\theta(\mu) + \frac{\lambda}{SA}\sum\limits_{s,a}\log_{\pi_\theta}\Cond{a}{s}\\
\theta	&\leftarrow& \theta + \eta \nabla L_\lambda(\theta)
\end{eqnarray}

Then, we get the following:
\begin{shaded}
\begin{theorem}[Global convergence: softmax + log barrier regularization]
S: #states, A:#actions, H: Horizon = 1/(1-\gamma), where \gamma is a function of \eps
Suppose \mu = uniform_s, and with appropriate settings of \lambda \& \eta, after \frac{S^4 A^2 H^6}{\eps^2} iterations we have, for all $s$,
\begin{equation}
V^0(s) \geq V^*(s) -\eps
\end{equation}
\end{theorem}
\end{shaded}
 
