\newcommand{\pt}[1]{\textcolor{red}{#1}}
\documentclass[12pt,reqno]{amsart}
\usepackage{amsmath}
\usepackage{cancel}
\usepackage{amssymb}
\usepackage[left=3cm,top=3cm,right=3cm,bottom=3cm]{geometry}
\usepackage{epsfig}
\usepackage[normalem]{ulem}
\usepackage[usenames,dvipsnames]{color}
\usepackage[textwidth=20mm]{todonotes}
\usepackage{hyperref}
\usepackage{eqnarray}
\usepackage{mathtools}
\usepackage{mathrsfs}
\newtheorem{case}{Case}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{tcolorbox}
\usepackage{hyperref}


\usepackage{soul}
\usepackage{tikz}
\usetikzlibrary{backgrounds}
\usetikzlibrary{intersections}

%\usepackage{pgfplots}
\usepackage{float}
\usepackage{xcolor}
\colorlet{shadecolor}{pink!69}
%\usepackage{floatrow}
% Table float box with bottom caption, box width adjusted to content. 
%Using floatrow instead broke my algorithm

\newtheorem{theorem}{Theorem}[section]
\newtheorem{question}{Question}
\newtheorem{defn}{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{namedtheorem}{Theorem}

\newcommand{\Gnp}{\G(n,p)}

\newcommand{\fancyP}{\mathscr{P}}

\newcommand{\Cond}[2]{\bigm( {#1} \bigm\vert {#2}\bigm)}
\newcommand{\G}{\mathbb{G}}
\newcommand{\FancyG}{\mathcal{G}}
\newcommand{\lineG}{\mathbb{G}}
\newcommand{\Ebf}{\textit{{\textbf{E}}}}
\newcommand\eps{\varepsilon}
\newcommand{\E}{\mathbb E}
\newcommand{\Z}{\mathbb Z}
\newcommand{\Cov}{\mathbb C\textrm{ov}}
\newcommand{\Var}{\mathbb V\textrm{ar}}
\newcommand{\Prob}{\mathbf{P}}
\newcommand{\Bin}{\mathrm{Bin}}
\newcommand{\Geom}{\mathrm{Geom}}
\newcommand{\Nn}{{\mathbb N}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\newcommand{\rh}[1]{\textcolor{blue}{#1}}
\usepackage{framed}

%usage: \begin{frshaded} \end{frshaded}


\title{Policy Gradient Methods, Curvature, and Distribution Shift}

\date{November 2, 2020}
\author{Sham Kakade}

\begin{document}
\maketitle

These are notes from a talk given by Dr. Sham Kakade on November 2nd on some rather high-level concepts in reinforcement learning. The talk can be accessed 
\href{http://www.fields.utoronto.ca/talks/Policy-Gradient-Methods-Curvature-and-Distribution-Shift}{\textbf{here}}.

In this write-up, I'll provide a very brief conceptual overview of some ideas that Dr. Kakade deals with before diving into his talk.

\section{What is reinforcement learning?}
\textbf{Reinforcement learning} is an area of machine learning that is concerned with the behaviour of agents who are concerned with maximizing utility. In a sense, it is paradigmatically divergent from \textbf{supervised} \& \textbf{unsupervised} learning, which focus upon recognizing patterns within a given domain.

Reinforcement learning is studied by scholars schooled in a multitude of different fields. Game theorists, control theorists, researchers in operations research, and statisticians, among others, study reinforcement learning. 

Basic reinforcement is modeled as a Markov decision process (MDP), consisting of four elements:
\begin{itemize}
\item A set of environment and agent states, $\mathcal{S}$. Can be either finite or infinite. We assume it's finite or countably infinite.
\item A set of actions $\mathcal{A}$ of an agent. Can be either finite or finite, but we typically assume it's finite.
\item A transition function $P :\mathcal{S} \times \mathcal{A} \rightarrow \delta(\mathcal{S})$, where $\delta(\mathcal{S})$ is the space of probability distributions over $\mathcal{S}$. $P \Cond{s'}{s,a}$. In other words, it's the \textit{probability simplex}.
\item $P_{a}(s,s') = \Pr\left(\mathcal{S}_{t+1}=s' \bigm\vert \mathcal{S}_{t} = s, a_{t} = a \right)$
\item A feward function $r: \mathcal{S} \times \mathcal{A} \rightarrow [0,1]$. This is the agent's reward for taking action $a$ at state $s$.
\item A discount factor $\gamma \in [0,1)$, which defines a horizon for the problem. Intuitively, $\gamma$ considers the tiem-scale of the pay-off (are we rewarding delayed gratification? In which case, $\gamma$ will be large. Are we rewarding immediate gratification? In which case, $\gamma$ will be small.)
\item An initial state distribution $\mu \in \delta(\mathcal{S})$, which specifies how the initial state $s_{0}$ is generated.
\end{itemize}

A Markov decision process occurs over discrete steps of time $t$ with can be characterized with a state $s_{t}$ and an agent's action $a_{t}$. The goal of the agent is to learn a policy $\pi: A \times S \rightarrow [0,1]$, $ \pi(a,s) = \Pr\Cond{a_{t} = a}{s_{t} = s} $
\section{What are policy methods?}
In a given MDP $M = (\mathcal{S},\mathcal{A}, P, r, \gamma, \mu)$, the agent interacts with the environment according to the following protocol: the agent starts at $s_{0} ~ \mu$ \& at each time step $t = 0,1,2,\ldots,$ the agent takes an action $a_{t} \in \mathcal{A}$, obtains the immediate reward $r_{t}(s_{t},a_{t})$, and observes the next state $s_{t+1}$ sampled according to $s_{t+1} ~ P \Cond{\cdot}{s_{t},a_{t}}$

The interaction record $\tau$ at time $t$,
\begin{equation*}
\tau_{t} = (s_{0},a_{0},r_{1} \ldots s_{t})5
\end{equation*}
is called the trajectory at time $t$ and includes the state at time $t$, $s_{t}$.

A policy specifies a decision-making strategy in which the agent is informed by the history of their observations (in this sense, a Markov decision process should be considered conceptually different from a Markov chain in terms of agnosticity to history). 

A policy is a (possibly randomized) mapping from a trajectory to an action. Thus,
\begin{equation*}
\pi: \mathcal{H} \rightarrow \delta(\mathcal{A})
\end{equation*}
, where $\mathcal{H}$ is the set of all possible trajectories (of all lengths) and $\delta \mathcal{A}$ is the space of all probability distributions over $\mathcal{A}$. A stationary policy $\pi : \mathcal{S} \rightarrow \mathcal{A}$ specifies a decision making strategy based solely on the current state (that is $a_{t} \sim \pi \Cond{\cdot}{s_{t}} $). A determinstic stationary policy is of the form $\pi: \mathcal{S} \rightarrow \mathcal{A}$

Now, we define \textbf{values} for general policies. For a fixed policy and a starting state $s_{0 = s}$, we define the value function $V_{m}^{\pi}: \mathcal{S} \rightarrow \mathbb{R}$ as the discounted sum of future rewards
\begin{equation}\label{eqn:Values}
V_{m}^{\pi}(s) = \E\left[ \sum\limits_{t = 0}^{\infty} \Cond{\gamma^{t}r(s_{t},a_{t})}{\pi, s_{0} = s}   \right]
\end{equation}

Where the expectation is computed with respect to the randomness of the trajectory, that is, the randomness in state-transitions and stochasticity of $\pi$. Since $r(s,a)$ is bounded between 0 \& 1, we have $0 \leq V_{m}^{\pi}(s) \leq 1/(1 - \gamma)$

Similarly, the action-value (or Q-value) function $Q_{M}^{\pi} : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ is defined as 
\begin{equation}\label{eqn:Q}
Q_{m}^{\pi} = \E \left[\sum\limits_{t = 0}^{\infty} \Cond{r(s_{t},a_{t})}{\pi, s_{0} = s, a_{0} = a}    \right]
\end{equation}
, which is also bounded from below by 0 and above by $1/(1-\gamma)$

\medskip

The optimization that the agent aims to solve is to maximize $V_{M}^{\pi}$ by finding a policy $\pi$. The agent thus wants to maximize
\begin{equation}\label{eqn:optvalues}
\max\limits_{\pi} V_{M}^{\pi}(s)
\end{equation}

\section{What was Dr. Kakade talking about?}
\begin{shaded}
\begin{defn}[The state space \& state variables]
A \textbf{state variable} is one of the set of variables that are used to describe the "state" of a dynamical system -- it describes enough of a system to determine its future behaviour in the absence of any external forces affecting the system.

The \textbf{state space} is the Euclidean space in which the variables on the axis are the state variables. It is a mathematical model of a physical system as a set of input, output,  and state varibles related by first-order differential equations or difference equations.
\end{defn}
\end{shaded}

The tabular dynamic programming approach is commonly taken to describe a reinforcement learning problem:

\begin{tabular}{|c|c|c|}
\hline
State $s$ & Action $a$ & State-action value $Q^{\pi}(s,a)$ \\
\hline
\ldots & \ldots & \ldots \\
\hline
\vdots & \vdots & \vdots \\
\hline
\ldots & \ldots & \ldots \\
\hline
\end{tabular}
The third column consists of a policy and gives the reward. It's a one-step look-ahead (that is, it gives the reward for the next step after). If we could magically compute it for each row, we could update our policy to be greedy and it will converge quickly. Unfortunately, the table is very large.

The RL approach is to generalize; instead of querying the entire table, we can take a sampling approach in order to find an optimal policy. Policy gradient methods are one such approach, which easily deal with large state/action spaces. The policy is directly parameterized (for instance, through a neural network) with the gradient easily computed with simulation-based methods. You don't need to know the model. In ML, it's common to directly optimize the quantity of interest. 

\textit{Why} this is possible is an open-question: the gradients in supervised learning (SL) and reinforcement learning are very different. We don't care about non-convexity much in SL, which isn't the case in RL. In RL, the regions can have exponentially small gradients, with much higher-order regions being flat.


\begin{thebibliography}{99}

%\bibitem{label} #name, #titleofwork, #publisher, #edition, #source
\bibitem{label} A.\ Agarwal, N.\ Jiang, S.M.\ Kakade, W.\ Sun; Reinforcement Learning: Theory and Algorithms; accessed October 27, 2020; available at \texttt{https://rltheorybook.github.io/}
\end{thebibliography}
\end{document}