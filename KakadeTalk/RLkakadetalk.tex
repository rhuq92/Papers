\newcommand{\pt}[1]{\textcolor{red}{#1}}
\documentclass[12pt,reqno]{amsart}
\usepackage{amsmath}
\usepackage{cancel}
\usepackage{amssymb}
\usepackage[left=3cm,top=3cm,right=3cm,bottom=3cm]{geometry}
\usepackage{epsfig}
\usepackage[normalem]{ulem}
\usepackage[usenames,dvipsnames]{color}
\usepackage[textwidth=20mm]{todonotes}
\usepackage{hyperref}
\usepackage{eqnarray}
\usepackage{mathtools}
\usepackage{mathrsfs}
\newtheorem{case}{Case}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{tcolorbox}
\usepackage[shortlabels]{enumitem}
\usepackage{hyperref}


\usepackage{soul}
\usepackage{tikz}
\usetikzlibrary{backgrounds}
\usetikzlibrary{intersections}

%\usepackage{pgfplots}
\usepackage{float}
\usepackage{xcolor}
\colorlet{shadecolor}{pink!69}
%\usepackage{floatrow}
% Table float box with bottom caption, box width adjusted to content. 
%Using floatrow instead broke my algorithm

\newtheorem{theorem}{Theorem}[section]
\newtheorem{question}{Question}
\newtheorem{defn}{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{namedtheorem}{Theorem}

\newcommand{\Gnp}{\G(n,p)}

\newcommand{\fancyP}{\mathscr{P}}

\newcommand{\Cond}[2]{\bigm( {#1} \bigm\vert {#2}\bigm)}
\newcommand{\cond}[2]{( {#1} \bigm\vert {#2})}
\newcommand{\smcond}[2]{( {#1} \vert {#2})}
\newcommand{\G}{\mathbb{G}}
\newcommand{\FancyG}{\mathcal{G}}
\newcommand{\lineG}{\mathbb{G}}
\newcommand{\Ebf}{\textit{{\textbf{E}}}}
\newcommand\eps{\varepsilon}
\newcommand{\E}{\mathbb E}
\newcommand{\Z}{\mathbb Z}
\newcommand{\Cov}{\mathbb C\textrm{ov}}
\newcommand{\Var}{\mathbb V\textrm{ar}}
\newcommand{\Prob}{\mathbf{P}}
\newcommand{\Bin}{\mathrm{Bin}}
\newcommand{\Geom}{\mathrm{Geom}}
\newcommand{\Nn}{{\mathbb N}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\newcommand{\rh}[1]{\textcolor{blue}{#1}}
\usepackage{framed}

%usage: \begin{frshaded} \end{frshaded}


\title{Policy Gradient Methods, Curvature, and Distribution Shift}

\date{November 2, 2020}
\author{Sham Kakade}

\begin{document}
\maketitle

These are notes from a talk given by Dr. Sham Kakade on November 2nd on some rather high-level concepts in reinforcement learning. The talk can be accessed 
\href{http://www.fields.utoronto.ca/talks/Policy-Gradient-Methods-Curvature-and-Distribution-Shift}{\textbf{here}}.

In this write-up, I'll provide a very brief conceptual overview of some ideas that Dr. Kakade deals with before diving into his talk.

\section{What is reinforcement learning?}
\textbf{Reinforcement learning} is an area of machine learning that is concerned with the behaviour of agents who are concerned with maximizing utility. In a sense, it is paradigmatically divergent from \textbf{supervised} \& \textbf{unsupervised} learning, which focus upon recognizing patterns within a given domain.

Reinforcement learning is studied by scholars schooled in a multitude of different fields. Game theorists, control theorists, researchers in operations research, and statisticians, among others, study reinforcement learning. 

Basic reinforcement is modeled as a Markov decision process (MDP), consisting of four elements:
\begin{itemize}
\item A set of environment and agent states, $\mathcal{S}$. Can be either finite or infinite. We assume it's finite or countably infinite.
\item A set of actions $\mathcal{A}$ of an agent. Can be either finite or finite, but we typically assume it's finite.
\item A transition function $P :\mathcal{S} \times \mathcal{A} \rightarrow \delta(\mathcal{S})$, where $\delta(\mathcal{S})$ is the space of probability distributions over $\mathcal{S}$. $P \Cond{s'}{s,a}$. In other words, it's the \textit{probability simplex}.
\item $P_{a}(s,s') = \Pr\left(\mathcal{S}_{t+1}=s' \bigm\vert \mathcal{S}_{t} = s, a_{t} = a \right)$
\item A feward function $r: \mathcal{S} \times \mathcal{A} \rightarrow [0,1]$. This is the agent's reward for taking action $a$ at state $s$.
\item A discount factor $\gamma \in [0,1)$, which defines a horizon for the problem. Intuitively, $\gamma$ considers the tiem-scale of the pay-off (are we rewarding delayed gratification? In which case, $\gamma$ will be large. Are we rewarding immediate gratification? In which case, $\gamma$ will be small.)
\item An initial state distribution $\mu \in \delta(\mathcal{S})$, which specifies how the initial state $s_{0}$ is generated.
\end{itemize}

A Markov decision process occurs over discrete steps of time $t$ with can be characterized with a state $s_{t}$ and an agent's action $a_{t}$. The goal of the agent is to learn a policy $\pi: A \times S \rightarrow [0,1]$, $ \pi(a,s) = \Pr\Cond{a_{t} = a}{s_{t} = s} $
\section{What are policy methods?}
In a given MDP $M = (\mathcal{S},\mathcal{A}, P, r, \gamma, \mu)$, the agent interacts with the environment according to the following protocol: the agent starts at $s_{0} ~ \mu$ \& at each time step $t = 0,1,2,\ldots,$ the agent takes an action $a_{t} \in \mathcal{A}$, obtains the immediate reward $r_{t}(s_{t},a_{t})$, and observes the next state $s_{t+1}$ sampled according to $s_{t+1} ~ P \Cond{\cdot}{s_{t},a_{t}}$

The interaction record $\tau$ at time $t$,
\begin{equation*}
\tau_{t} = (s_{0},a_{0},r_{1} \ldots s_{t})5
\end{equation*}
is called the trajectory at time $t$ and includes the state at time $t$, $s_{t}$.

A policy specifies a decision-making strategy in which the agent is informed by the history of their observations (in this sense, a Markov decision process should be considered conceptually different from a Markov chain in terms of agnosticity to history). 

A policy is a (possibly randomized) mapping from a trajectory to an action. Thus,
\begin{equation*}
\pi: \mathcal{H} \rightarrow \delta(\mathcal{A})
\end{equation*}
, where $\mathcal{H}$ is the set of all possible trajectories (of all lengths) and $\delta \mathcal{A}$ is the space of all probability distributions over $\mathcal{A}$. A stationary policy $\pi : \mathcal{S} \rightarrow \mathcal{A}$ specifies a decision making strategy based solely on the current state (that is $a_{t} \sim \pi \Cond{\cdot}{s_{t}} $). A determinstic stationary policy is of the form $\pi: \mathcal{S} \rightarrow \mathcal{A}$

Now, we define \textbf{values} for general policies. For a fixed policy and a starting state $s_{0 = s}$, we define the value function $V_{m}^{\pi}: \mathcal{S} \rightarrow \mathbb{R}$ as the discounted sum of future rewards
\begin{equation}\label{eqn:Values}
V_{m}^{\pi}(s) = \E\left[ \sum\limits_{t = 0}^{\infty} \Cond{\gamma^{t}r(s_{t},a_{t})}{\pi, s_{0} = s}   \right]
\end{equation}

Where the expectation is computed with respect to the randomness of the trajectory, that is, the randomness in state-transitions and stochasticity of $\pi$. Since $r(s,a)$ is bounded between 0 \& 1, we have $0 \leq V_{m}^{\pi}(s) \leq 1/(1 - \gamma)$

Similarly, the action-value (or Q-value) function $Q_{M}^{\pi} : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ is defined as 
\begin{equation}\label{eqn:Q}
Q_{m}^{\pi} = \E \left[\sum\limits_{t = 0}^{\infty} \Cond{r(s_{t},a_{t})}{\pi, s_{0} = s, a_{0} = a}    \right]
\end{equation}
, which is also bounded from below by 0 and above by $1/(1-\gamma)$

\medskip

The optimization that the agent aims to solve is to maximize $V_{M}^{\pi}$ by finding a policy $\pi$. The agent thus wants to maximize
\begin{equation}\label{eqn:optvalues}
\max\limits_{\pi} V_{M}^{\pi}(s)
\end{equation}

\section{What was Dr. Kakade talking about?}
\begin{shaded}
\begin{defn}[The state space \& state variables \& gradients]
A \textbf{state variable} is one of the set of variables that are used to describe the "state" of a dynamical system -- it describes enough of a system to determine its future behaviour in the absence of any external forces affecting the system.

The \textbf{state space} is the Euclidean space in which the variables on the axes are the state variables. It is a mathematical model of a physical system as a set of input, output,  and state varibles related by first-order differential equations or difference equations.

The \textbf{gradient} $\nabla f(p)$ of a scalar-valued differential equation $f$ of several variables $(x_1,x_2, \ldots, x_n)$ is given as:
\begin{equation}
\nabla f(p) = \begin{bmatrix}
\frac{\delta f}{\delta x_1}(p)\\
\vdots\\
\frac{\delta f}{\delta x_n}(p)
\end{bmatrix}
\end{equation}
\end{defn}
\end{shaded}

The tabular dynamic programming approach is commonly taken to describe a reinforcement learning problem:
\newline
\begin{tabular}{|c|c|c|}
\hline
State $s$ & Action $a$ & State-action value $Q^{\pi}(s,a)$ \\
\hline
\ldots & \ldots & \ldots \\
\hline
\vdots & \vdots & \vdots \\
\hline
\ldots & \ldots & \ldots \\
\hline
\end{tabular}
\newline
	The third column consists of a policy and gives the reward. It's a one-step look-ahead (that is, it gives the reward for the next step after). If we could magically compute it for each row, we could update our policy to be greedy and it will converge quickly. Unfortunately, the table is very large.

The RL approach is to generalize; instead of querying the entire table, we can take a sampling approach in order to find an optimal policy. Policy gradient methods are one such approach, which easily deal with large state/action spaces. The policy is directly parameterized (for instance, through a neural network) with the gradient easily computed with simulation-based methods. You don't need to know the model. In ML, it's common to directly optimize the quantity of interest. 

\textit{Why} this is possible is an open-question: the gradients in supervised learning (SL) and reinforcement learning are very different. We don't care about non-convexity much in SL, which isn't the case in RL. In RL, the regions can have exponentially small gradients, with much higher-order regions being flat.

In this talk we look at global convergence of RL methods and provide generalization guarantees of nonconvex policy gradient methods.

We will start with a simple test case of small state spaces and exact gradients before moving to large state spaces.

\begin{shaded}
\begin{defn}
A non-convex space is a space which isn't convex. That is, if for all points $x,y \in \mathbb{R}$, and any $\lambda \in [0,1]$, it is true that $\lambda x + (1-\lambda)y \in \mathbb{R}$, then $\mathbb{R}$ is convex.
\end{defn}
\end{shaded}

An iterative method is called \textbf{locally convergent} if successive approximations are guaranteed to approach when the initial approximation is sufficiently close to the solution. An iterative method that converges for an arbitrary initial approximation is called \textbf{globally convergent}.

This talk provides global convergence \& generalization guarantees of non-%convex policy gradient methods. 

They start with small state spaces with exact gradients before moving to large state spaces. Dealing with large state spaces means we need to deal with generalization (obviously) and distribution shift.

\section{Small state spaces \& the "softmax" policy class}
The softmax policy is the simplest way to parameterize the simplex

What's the simplex?
\begin{shaded}
\begin{defn}
        A simplex is a generalization of the notion of a triangle or tetrahedron to arbitrary dimensions. In the context of machine-learning, we can consider the feasible region (that is, the set of possible strategies) to be a simplex. Navigating the simplex, then, means finding the optimal solution.
\end{defn}
\end{shaded}
A policy class is, essentially, a set of policies.

Softmax policies are of the form
\begin{equation}
\pi_\theta\theta{\Cond{a}{s}}=\frac{\displaystyle \exp(\theta_s,a)}{\displaystyle \sum_{a'}\exp(\theta_s,a')}
\end{equation}
The complete class contains every stationary policy (and a stationary policy is a policy that does not change over time.).

The policy optimization problem $\max\limits_{\theta} V^{\pi_{\theta}}(s_0)$ is non-convex. If we can't find an optimal policy class with this simple problem, how can we find them for a more complicated problem?

We're going to avoid sampling and assume we can get the exact gradients (that is, $\nabla V^{\pi_0}(s_0)$ is given exactly.).

Before we do that, we need to define the following:

\begin{shaded}
\begin{defn}[Gradient Descent]
Gradient descent is a first-order (that is, it has at most linear error) iterative optimization algorithm for finding the local minumum of a differentiable function.
\end{defn}
\end{shaded}

The softmax policy is the simplest way to parameterize the simplex (that is, it converts the one-step look-ahead values of a state \& action pair into action probabilities). It characterizes the function as a logistic function. However, the rate could be exponentially slow in terms of state and the softmax can have very flat gradients (meaning if you go to some corner of the simplex, the gradient could be very slow, so moving away from the corner can take a very long time.). This means that finding the optimal policy could be exponentially slow in the number of states

What if we try some sort of regularization-penalty to bias ourselves from getting these flat gradients (which, for the record, is very possible if the policy is deterministic)? The simplest way to do this is to use log-barrier regularization.

Instead of doing gradient descent on the spread-out measure, we can add a penalty that pushes us away from determinsitc policies. The average-log probability stops us from forming a policy that's too deterministic. This leads us to optimize the following:
\begin{eqnarray}
L_\lambda(\theta) &=& V^\theta(\mu) + \frac{\lambda}{SA}\sum\limits_{s,a}\log_{\pi_\theta}\Cond{a}{s}\\
\theta  &\leftarrow& \theta + \eta \nabla L_\lambda(\theta)
\end{eqnarray}

Then, we get the following:
\begin{shaded}
\begin{theorem}[Global convergence: softmax + log barrier regularization]
\par
S: \# states, A:\# actions, H: Horizon = 1/(1-$\gamma$), where $\gamma$ is a function of $\eps$
\par
Suppose $\mu$ = uniform$_{s}$, and with appropriate settings of $\lambda \& \eta$, after $\frac{S^4 A^2 H^6}{\eps^2}$ iterations we have, for all $s$,
\begin{equation}
V^0(s) \geq V^*(s) -\eps
\end{equation}
\end{theorem}
\end{shaded}

To put it succinctly: they found that the policy gradient converges in polynomial time, which is pretty quick for this simple case, as it's non-convex.

What's going on? The log-barrier plus the uniform distribution helps with conditioning problems. The log barrier regularization is the same as KL-regularization (different from entropy regularization) which allows for better convergence to a deterministic strategy.

Another approach is to warp where we move along the simplex by preconditioning. Can we stretch the geometry out so we can move quickly when the gradient becomes flat?
        We can do this with the Fisher information matrix (which measures the amount that an observable random variable $X$ tells us about an unknown parameter $\theta$ of a distribution that models $X$).
        For every state we have a Fisher matrix. We can take the expected Fisher metric over the states we tend to visit under the current distribution. At this point, it's the Fisher matrix of the trajectories we follow. 
        
        When we run gradient descent on this preconditioner, the update rule looks like a "soft" policy iteration update rule, where the probability of an action is relative to the exponent of Q. So what's happening with this non-convex update rule? There's no dependence on the starting distribution because of the preconditioning, so there's a source of mystery.

Looking at the convergence rate, after the learning rate is appropriately set, for all states we get that we converge to the optimal state at the rate of 1/T (where T is the number of iterations). This has no dependence on the number of states, A, or the starting measure $\mu$. This is pretty fast (we usually have $1/\sqrt{T}$).

We have some tools to think about these flat gradients. We understand the curvature issue. With exact gradients, a tabular approach converges, and with a preconditioner we can converge very quickly without dependence on S and A.

But there are a few more questions:
\begin{enumerate}

\item How do we think about approximation/generalization (which isn't an issue in supervised learning)? \\

\item How do we think about $\mu$ in an infinite state space?\\

\end{enumerate}
\section{Large state spaces}

We'll use what we know about the preconditioned case, where we found really fast convergence without dependance on $S \& A$.

If we want to apply our ideas to more complex problems than maze searching (which is characterized by a small state space) such as robotics, healthcare data, interacting with the world, games, etc. 

\begin{shaded}
A question relevant to my research: can we consider strategies in vertex-pursuit games as Markov Decision Processes? What does that say about the state-space of possible strategies? Might variations of games differ? 
\end{shaded}

These more complex problems require us to think about approximation and generalization.

We started with a soft-max policy class
%What's important about it?
Most policy classes we use are of the form:
\begin{equation}
\pi_\theta \cond{a}{s} \propto \exp(f_\theta(s,a))
\end{equation}
Meaning, they're the log of some function ($e^{\ln(f_\theta))}$). It is just a tabular paramaterization. He wrote this:
\begin{equation}
f_\theta(s,a)=\theta_{s,a}
\end{equation}
A log-linear parameterization has some features:
\begin{equation}
f_\theta(s,a) = \overrightarrow{\theta} \cdot \overrightarrow{\phi}(s,a) \; \textrm{where} \; \overrightarrow{\phi} \in \mathbb{R}^d
\end{equation}
Another natural policy is a neural policy class where $f_\theta(s,a)$ is a neural network.

We can make $f$ log-linear. It may not even contain the optimal policy. We have a feature mapping, where the feature vector is $\phi(s,a) \in \mathbb{R}^d$ \& $\pi_\theta\cond{a}{s} \propto \exp(\theta,\phi_{s,a})$. We want the dimension $d$ to be smaller than the number of states and actions. 


At iteration $t$ we use the natural policy-gradient (NPG) update rule:
\begin{equation}
\theta \leftarrow \eta F(\theta)^{-1}\nabla V^\theta(s_0)
\end{equation}
Which is actually equivalent to the "soft"-approximate policy iteration update:
\begin{enumerate}[1.]
\item Approximate $Q^\theta$ with the features:
\begin{equation}
w_\star \in \arg \min_{w} E_{s,a\sim d\smcond{\cdot}{\pi,\mu}}\left[ (Q^\theta (s,a) - w \cdot \phi_{s,a})^2 \right]
\end{equation} where $d\cond{\cdot}{\pi,\mu}$ is the "on-policy" distribution starting from $s_0,a_0 \sim \mu$. $\mu$ is an on-policy coverage. We can solve the above equation with regression.\\
\item \begin{equation}
\pi\cond{a}{s} \leftarrow \frac{\pi\cond{a}{s} \exp(w_\star \cdot \phi_{s,a}}{Z_{s}}
\end{equation}
where $Z_{s}$\\
\end{enumerate}
which is a sample-based approach, which is interesting since it's saying that a gradient-update rule looks like regression. It works since in the NPG you're solving a linear system with a preconditioner, which looks like the update rule in 2. You're finding the best fit $Q^\theta$ with your features and then you'll update your policies by being $e^{\hat{Q}}$, since you can think of $\exp(w_\star,\phi_{s,a})$ as being the best approximation to $Q$ with the features you've got.

It's a clean update rule and a soft-way of doing the policy gradient. If we made $\eta$ infinity we'd get a greedy update rule, but the soft approach is far more incremental, which is good since it's more like hill-climbing. 

Instead of exact updates we can use samples, meaning we use samples for our update rule and for $Q$. This is like putting supervised learning into our problem, since computing $\hat{Q}$ is just regression. If we're generalizing in some sense, can we do RL well? Can we handle the curse of dimensionality? 

It's important to talk about the natural policy-gradient, since that's what gives us the dimension-free convergence that we like.

Let's begin with some assumptions:
\begin{enumerate}
\item Realizability: Suppose that $Q^\theta(s,a)$ is a linear function in $\phi(s,a)$ (it lives in the span of these features). This means our regression model is correct. There's a connection between optimization and value-based assumptions.\\
\item Supervised learning error: Our estimate $\hat{w}^{t}$ has bounded regression error (say, due to sampling) 
\begin{equation}
E_{s,a\sim \smcond{d}{\pi,\mu}}\left[ (Q^\theta(s,a) - \hat{w}^t \cdot \phi_{s,a})^2 \right] \leq \eps_{stat}
\end{equation}
where $\eps_{stat}$ is our statistical error (which goes to 0 as we get more samples).\\
\item Conditioning (i.e. "feature coverage"): $||\phi_{s,a}||^{2} \leq 1$ and define
\begin{equation}
\kappa = 1/ \sigma_{\min} (E_{s,a\sim \mu} \left[ \phi_{s,a} \phi_{s,a}^{T} \right]
\end{equation}
We can't expect this method to work if we start at a given starting state because of the flat-gradient problem. So we suppose our starting distribution has coverage over the covariance matrix of the features.\\
\end{enumerate}
\break
\begin{shaded}
\begin{theorem}

A: \# actions, H: Horizon = 1/(1-$\gamma$), Norm bound: $||\hat{w}^{t}|| \leq W$
\newline
After $T$ the iterations, the NPG algorithm returns a $\pi$ such that
\begin{equation}
V^{T}(\rho) \geq V^{\star}(\rho) - HW \sqrt{\frac{2 \log A}{T}} + \sqrt{4AH^{3}\kappa\eps_{stat}}
\end{equation}
\end{theorem}
\end{shaded}

\bigskip
\bigskip
\bigskip
\subsection{What happens when we drop the realizability assumption?}
It's a lot to ask for $Q^{\theta}$ to be approximated and estimated with $\phi$.

Let's revisit the sample-based procedure. The ways that approximation and estimation error appear in the RL setting is a bit subtle and different than how we think about these things in supervised learning. 

The ways in which approximation and estimation error appear in RL is different than supervised learning.

$L$ is a loss function. For a state-action distribution $\nu$ we define
\begin{equation}
L(w;\theta,\nu) := E_{s,a \sim \nu}\left[ (Q^{\pi_\theta}(s,a) -w \cdot \phi_{s,a})^2 \right]
\end{equation}
$\nu$ is the measure we use in our optimization. It's how we get our sample. We're minimizing $E_{s,a \sim \nu}$.
%when we get samples
The NPG update 
\begin{enumerate}
\item Approximate $Q^\theta$ with the features:
\begin{equation}
\hat{w}^t \approx \arg \min_w \hat{L}(w; \theta, d\smcond{\cdot}{\pi,\mu})
\end{equation}
where $d\smcond{\cdot}{\pi,\mu}$ is an "on-policy" distribution starting from $s_0, a_0 \sim \mu$. If it's not realizable, the minimum error of $\hat{L}$ may not be zero.\\
\item Policy update: $\Cond{a}{s} \leftarrow \Cond{a}{s}\exp(w_\star \cdot \phi_{s,a}) /Z_s$ where $Z_s$ is the normalizing constant.\\
\end{enumerate}
$L$ is doing regression but we're assuming that there's approximation error.

So we make two assumptions: 
\begin{enumerate}
\item Supervised learning error: Suppose the excess risk and approx error are bounded as: 
\begin{eqnarray}
&& L(w^{(t)}; \theta^{(t)}, d^{(t)}) - L(w^{(t)_\star}; \theta^{(t)}, d^{(t)}) \leq \eps_{\textrm{stat}} \\
&& L(w^{(t)}; \theta^{(t)}, d^{(t)}) \leq \eps_{\textrm{approx}}\\
\end{eqnarray} 
\item Conditioning (i.e. "feature coverage"): $||\phi_{s,a}||^{2} \leq 1$ and define
\begin{equation}
\kappa = 1/ \sigma_{\min} (E_{s,a\sim \mu} \left[ \phi_{s,a} \phi_{s,a}^{T} \right]
\end{equation}\\
\end{enumerate}
We know $\eps_{stat}$ goes to zero, since it will as we get zero as we get more data. $\eps_{approx}$ won't go to zero, since it's defined according to the approximation error from your current distribution. We keep updating our policy and the distribution keeps changing, but low approximation under one distribution doesn't apply to other distributions.

\begin{shaded}
\begin{theorem}
A: \# actions, H: Horizon = 1/(1-$\gamma$)
\newline
After $T$ iterations, the NPG algorithm returns a $\pi$ such that:
\begin{equation}
V^{(T)}(s_0) \geq V^{\star}(s_0) - HW \sqrt{\frac{2 \log A}{T}} + \sqrt{4AH^3(\kappa \cdot \eps_{\textrm{stat}}} + ||\frac{d^\star}{\mu}||_\infty \cdot \eps_{\textrm{approx}}
\end{equation}
\end{theorem}
\end{shaded}
where $d^\star$ is the state-acttion distribution of any comparator policy $\pi^{\star}$ (we wanna do as well as $\pi^\star$). We need our approximation error to be akin to $\pi^\star$'s. Therefore we need the approximation error to be small under another distribution, so if there approximation error is large we're screwed. Hopefully our features allow us to have low approximation error. This is definitely weaker than realizability (note: it's the $\infty$ ration that's very problematic).
\begin{thebibliography}{99}

%\bibitem{label} #name, #titleofwork, #publisher, #edition, #source
\bibitem{label} A.\ Agarwal, N.\ Jiang, S.M.\ Kakade, W.\ Sun; Reinforcement Learning: Theory and Algorithms; accessed October 27, 2020; available at \texttt{https://rltheorybook.github.io/}
\end{thebibliography}
\end{document}